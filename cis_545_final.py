# -*- coding: utf-8 -*-
"""CIS 545 final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MoeFZ5EgvAnsyj0zms68N9CDXVP5AIOH

### **Group members: Xuanyang Wang, Bowen Tan**

## **Introduction**

###*Background*
This spring, a small discussion group in Reddit called WallStreetBets invoked great impact on the stock market. The entire market was shocked by the prediction and operations on certain stocks like GME and AMC led by some users within this discussion group. We therefore decided to use data analysis skill to find out what do they talk about in their discussions.

###*Project Goal*
In this project, we are trying to look at the data set that contains posts from Wall Street Bets. We are aiming to find out the reason behind high-score and high-comment posts, and find out trend of posts within the group. We also want to train models that help predict what kind of posts would users in this group most likely to view.

###*Data set*
We found two separate datasets on kaggle that contain the posts from r/wallstreetbets. The larger one is 200MB, the smaller one is 30MB. We are mainly studying the smaller data set in the following EDA.
"""

import os
from google.colab import drive

# Mount google drive
DRIVE_MOUNT='/content/gdrive'
drive.mount(DRIVE_MOUNT, force_remount=True)

# create folder to write data to
CIS545_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS545_2021')
FOLDER=os.path.join(CIS545_FOLDER, 'final')
os.makedirs(FOLDER, exist_ok=True)

import warnings
warnings.filterwarnings("ignore")

! pip install -q kaggle
! mkdir ~/.kaggle
! cp /content/gdrive/MyDrive/CIS545_2021/final/kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# ! kaggle datasets list

# !kaggle datasets download -d unanimad/reddit-rwallstreetbets -p /content/gdrive/MyDrive/CIS545_2021/final
# !kaggle datasets download -d gpreda/reddit-wallstreetsbets-posts -p /content/gdrive/MyDrive/CIS545_2021/final

# !unzip /content/gdrive/MyDrive/CIS545_2021/final/reddit-rwallstreetbets.zip -d /content/gdrive/MyDrive/CIS545_2021/final/data_1
# !unzip /content/gdrive/MyDrive/CIS545_2021/final/reddit-wallstreetsbets-posts.zip -d /content/gdrive/MyDrive/CIS545_2021/final/data_2

import pandas as pd

data_1_df = pd.read_csv('/content/gdrive/MyDrive/CIS545_2021/final/data_1/r_wallstreetbets_posts.csv')#bigger 
data_2_df = pd.read_csv('/content/gdrive/MyDrive/CIS545_2021/final/data_2/reddit_wsb.csv')# smaller

!pip install pandasql

import pandasql as ps
import nltk

query = '''
SELECT * FROM data_2_df
WHERE
title IS NOT NULL
AND
score IS NOT NULL
AND
id IS NOT NULL
AND
comms_num IS NOT NULL
AND
created IS NOT NULL
AND
timestamp IS NOT NULL
'''

data_2_clean = ps.sqldf(query, locals())

data_2_clean

"""## **EDA**

###**Data Set 2**

In this dataset, we noticed that even though it contains the 'body' of each posts, more than 50% of the body is just invalid as shown in the picture below:

![bd_col.PNG](data:image/png;base64,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)

We therefore decide to change the 'body' column as binary value (has body or not) and mainly use the title column to identify the content within the post.
"""

#Change body to 'have_body'

print(data_2_clean['body'])

def to_binary(input):
  if input is None:
    return 0
  else:
    return 1

data_2_clean['have_body'] = data_2_clean['body'].apply(to_binary)

"""We first convert the timestamp into month and year. As we can see, all the posts from data set 1 are from 2021 between end of January and mid April. """

data_2_clean['datetime'] = pd.to_datetime(data_2_clean['timestamp'])
data_2_clean['year'] = data_2_clean['datetime'].dt.year
data_2_clean['month'] = data_2_clean['datetime'].dt.month_name()
data_2_clean['day'] = data_2_clean['datetime'].dt.day
data_2_clean = data_2_clean.drop(columns=['timestamp', 'body', 'url'])
data_2_clean

"""As we plot the count of posts against the month, we found that the number of posts gradually decreases over the year, and there are some random posts from september. We therefore decide to drop those data since we are focusing on the posts in WSB between January and April."""

import matplotlib
import matplotlib.pyplot as plt
from matplotlib import cm
import glob
import seaborn as sns
data_2_clean['month'] = data_2_clean['datetime'].dt.month_name()
plot1 = sns.catplot(data=data_2_clean, x='month', kind='count')
plot1.set_xticklabels(rotation=90)
plot1 = plt.title('Post Count vs Month')

"""After we dropped the September data, we replot the Post Count against Month. It seems that January and Febuary are the month when most posts came out."""

data_2_clean = data_2_clean[data_2_clean['month'] != 'September']
plot1 = sns.catplot(data=data_2_clean, x='month', kind='count')
plot1.set_xticklabels(rotation=90)
plot1 = plt.title('Post Count vs Month')

"""Since the score and comment numbers are having great range as we observed from the data set above, we decided to take log scale on both the data here. After that, we plot the distribution of them and found that it is heavily headed. This is mostly likely because most of the posts does not get comments. The scores are in general higher because users tend to like/dislike a post but would only comment on really interesting/controversy ones."""

import numpy as np
data_2_clean['score_log'] = data_2_clean['score'].add(1).apply(np.log)
data_2_clean['comms_log'] = data_2_clean['comms_num'].add(1).apply(np.log)
score_log_plot = data_2_clean['score_log'].plot.kde()
comms_log_plot = data_2_clean['comms_log'].plot.kde()

"""Then, we plotted the score_log and comms_log verses the month as box plots. As we can observe, the fewer the post in a month, the higher the 'quality' of those posts would be, since they tend to have higher score and number of comments on average."""

plot2 = sns.boxplot(data=data_2_clean, x='month', y='score_log')
plot2 = plt.title('Score_log vs Month')

plot3 = sns.boxplot(data=data_2_clean, x='month', y='comms_log')
plot3 = plt.title('Comms_log vs Month')

"""In plot4, we can see that score and number of comments are postively related in general (which would be further verified in later part), and we can see that the post with highest score was in January, and the post with most comments was in March."""

plot4 = sns.relplot(x='score_log', y='comms_log', hue='month', data=data_2_clean)

"""We then aim to find the TOP 10 post with most mean score and comment numbers:"""

# Post with most awards / score
# no author for data_2

query = '''
SELECT id, score as score, sum(comms_num) as sum_comms FROM data_2_clean
GROUP BY id
ORDER BY sum_comms DESC
'''

data_2_author = ps.sqldf(query, locals())

#Score for Top 10 commented posts in data set 2
print(data_2_author.head(10))

"""Similar to the previous part, this is looking at the TOP 10 posts with highest score"""

# Post with most awards / score

query = '''
SELECT id, title, score, comms_num FROM data_2_clean
ORDER BY score DESC
'''

data_2_post = ps.sqldf(query, locals())

#Top 10 posts
print(data_2_post.head(10))

"""To further study the correlation between score and comment numbers, we built up the heatmap based on the correlation matrix of score and comment numbers in data set 2."""

vec1 = data_2_post['score'].to_list()
vec2 = data_2_post['comms_num'].to_list()

import numpy as np

corr = np.corrcoef(vec1, vec2)
print(corr)

# use a heatmap to show the relationship among those features
sns.set()

fig = plt.figure()
plt.title('corresondence among features')
sns_plot = sns.heatmap(corr)
plt.show()

"""Next, we decided that it is important to look into the words within the title and found out their effects on score/comment numbers. Before doing so, we first need to normalize the score and comment number since they were originally having huge range. We choice to use min-max normalization in this part."""

# Current dataframe is data_2_post, count the frequency and add the score at the same time
# Normalize it using min-max normalization
from sklearn import preprocessing

x = data_2_post[['score','comms_num']].values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
norm = pd.DataFrame(x_scaled)
preprocessed = pd.concat([data_2_post['title'], norm], axis=1)
preprocessed = preprocessed.rename(columns={0:"score", 1:"comms_num"})
preprocessed.dropna()

"""Then, we get rid of all the special characters in each title, and only leave valid words within each title."""

#Get rid of all the special characters in each title
import regex as re
preprocessed['title'].astype(str)
preprocessed['title'] = preprocessed['title'].apply(lambda x: re.sub(r'[^\w]', ' ', x))
preprocessed['title']

"""It is now time to count the word frequency! In the following part, we first set the stopwords for all the titles, and weight the word frequency based on the score of each post."""

from collections import defaultdict
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
word_freq = defaultdict(lambda: [0, 0]) 
all_stopwords = stopwords.words('english')
stopword_list = ['i', 'we','us', 'a','like','play', 'our', 'do']
all_stopwords.extend(stopword_list)

# looping is now over both the text and the numbers
for index, row in preprocessed.iterrows(): 
  text_tokens = word_tokenize(row['title'])
  row['title'] = [word for word in text_tokens if not word.lower() in all_stopwords]
  for word in row['title']: 
      # same as before
      word_freq[word][0] += 1 
      # new line, incrementing the numeric value for each word
      word_freq[word][1] += row['score']

columns = {0: 'abs_freq', 1: 'wtd_freq'}

abs_wtd_df = pd.DataFrame.from_dict(word_freq, orient='index') \
             .rename(columns=columns) \
             .sort_values('wtd_freq', ascending=False) \
             .assign(rel_value=lambda df: df['wtd_freq'] / df['abs_freq']) \
            .round()

abs_wtd_df.style.background_gradient(low=0, high=.7, subset=['rel_value'])

"""Here, we make a statistic of absolute and weighted frequency. To add more information, we counted the cumulative of both frequencies, and the percentage of each word. As we can see from the result, GME and AMC are both optaining top 5 heat words in wallstreetbets in the first 4 month of 2021."""

abs_wtd_df.insert(1, 'abs_perc', value=abs_wtd_df['abs_freq'] / abs_wtd_df['abs_freq'].sum())
abs_wtd_df.insert(2, 'abs_perc_cum', abs_wtd_df['abs_perc'].cumsum())
abs_wtd_df.insert(4, 'wtd_freq_perc', abs_wtd_df['wtd_freq'] / abs_wtd_df['wtd_freq'].sum())
abs_wtd_df.insert(5, 'wtd_freq_perc_cum', abs_wtd_df['wtd_freq_perc'].cumsum())
abs_wtd_df.style.background_gradient(low=0, high=0.8)
abs_wtd_df.head(20)

"""###**Data Set 1**
Now, we look at Data set 1. This is the larger data set with 200MB, we therefore did not do deep EDA on this one.
"""

# clear some useless data

query = '''
SELECT * FROM data_1_df
WHERE
id IS NOT NULL
AND
title IS NOT NULL
AND
score IS NOT NULL
AND
author IS NOT NULL
AND
created_utc IS NOT NULL
AND
num_comments IS NOT NULL
'''

data_1_clean = ps.sqldf(query, locals())

# if a post is removed by someone, then it can be classified as removed, which is much easier for a computer to understand than some names.

data_1_clean['removed'] = data_1_clean['removed_by'].apply(to_binary)

print(data_1_clean[['author', 'author_flair_text','removed', 'total_awards_received']])

"""We decide to drop all the posts taht are already removed, which have \[deleted] as title."""

# also clear the posts from users who deleted their account

query = '''
SELECT * FROM data_1_clean
WHERE author IS NOT '[deleted]'
'''

data_1_not_del = ps.sqldf(query, locals()).fillna(0)

data_1_not_del

print(data_1_df.columns)

# Author with most awards / score

query = '''
SELECT author, count(score) as num_post, avg(score) as mean_score,avg(num_comments) as mean_comms, avg(removed) as ratio_removed, avg(total_awards_received) as mean_awards FROM data_1_not_del
GROUP BY author
ORDER BY mean_score DESC
'''

data_1_author = ps.sqldf(query, locals())
data_1_author = data_1_author.fillna(0)

data_1_author

# feature vectors

vec1 = data_1_author['num_post'].to_list()
vec2 = data_1_author['mean_score'].to_list()
vec3 = data_1_author['mean_comms'].to_list()
vec4 = data_1_author['ratio_removed'].to_list()
vec5 = data_1_author['mean_awards'].to_list()

import numpy as np

corr = np.corrcoef([vec1, vec2, vec3, vec4, vec5])
# print(corr)

# use a heatmap to show the relationship among those features

fig = plt.figure()
plt.title('corresondence among features')
sns_plot = sns.heatmap(corr)
plt.show()

query = '''
SELECT * FROM data_1_author
ORDER BY num_post DESC
'''

data_1_num_post = ps.sqldf(query, locals())

data_1_num_post

"""### **Modeling**

In this part we are going to explore the dataset with several models to reveal the interrelationship between the features and the post itself. Firstly we applied a tf-idf as preprocessing to get the weighted word frequency of the keywords for further exploration
"""

from nltk.tokenize import sent_tokenize
import nltk.data
from nltk.tokenize import word_tokenize
nltk.download('punkt')
sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

"""Firstly we analized dataset2, the smaller dataset containg about 45k posts."""

# Keyword with most scores 
# TF-IDF (keyword scoring model)
# Dataset 2

from sklearn_pandas import DataFrameMapper, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report
X = data_2_post['title'].fillna('')
from nltk.corpus import stopwords



import numpy as np
import random
import os
import torch

# set the random seed to ensure everyone can get the same result with the code
def seed(seed=0):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.enabled = False

seed()

# use max_df to filter common words like and, or, ect, and min_df to filter worthless words that showed up for less than a certain times
all_stopwords = stopwords.words('english')

vectorizer = TfidfVectorizer(max_df=0.1, min_df=0.0005, stop_words=all_stopwords)
features = vectorizer.fit_transform(X)
words = vectorizer.get_feature_names()

print(words)

# original dictionary length: 19861
# after some filering
print(len(words))

# split the dataset
label = data_2_post[['score', 'comms_num']]

X_train,X_test,y_train,y_test = train_test_split(features,label,test_size=0.2,train_size=0.8, random_state = 0)

del features

X_train.shape

"""Since the feature is high dimensional, i firstly tried apply pca upon it to see if there will be any benefit, however it seems that pca will only make things worse, since we got a correspondense of 0.65 between prediction and ground truth without pca, but a correspondense of 0.06 with it.(Ridge regression) Therefore no PCA will be conducted to the features in our further experiment."""

from sklearn.decomposition import PCA
from sklearn.decomposition import SparsePCA

# pca = PCA(n_components=32)
# pca = SparsePCA(n_components=32, random_state=0)
# pca.fit(X_train.toarray())
# X_train = pca.transform(X_train.toarray())
# X_test = pca.transform(X_test.toarray())

# preprocess the data and prepare them for the models
X_train = X_train.toarray()
X_test = X_test.toarray()

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

score_train = y_train[:,0]
comments_train = y_train[:,1]

score_test = y_test[:,0]
comments_test = y_test[:,1]

"""After some simple preprocessing, I applied ridge regression upon the dataset to see if we can predict the score that a post would get given its title using the ridge regression algorithm"""

# ridge regression
# predict score/ number of response

ri = Ridge(alpha=0.5)
ri.fit(X_train, y_train)
y_pred = ri.predict(X_test)

corr_1 = np.corrcoef(y_pred[:,0],y_test[:,0])[0,1]
corr_2 = np.corrcoef(y_pred[:,1],y_test[:,1])[0,1]
# print(corr_1, corr_2)
print('Correlation coefficients of the predicted score a post will get and the ground truth:')
print(corr_1)
print('Correlation coefficients of the number of responses a post will get and the ground truth:')
print(corr_2)

"""Since its a regression task, we dicide to use the  correlation coefficients as the evalution matric to evaluate how good the model is. It seems that the title, or at least the keywords have little to do with the score of a post, but have great influence upon the number of responses that a post will get. Here are some examples."""

# lowest predicted posts: no.5201 5738 7608
# highest predicted posts: no.3393 7718 7290

print('posts that is predicted to get a lot of responses:')

for idx in [3393, 7718, 7290]:
  post = data_2_post.loc[idx][['title', 'score',	'comms_num']].to_list()
  print('title:', post[0],'score:', post[1],'responses:',post[2])
  print('\n')
  print('predicted score:', int(y_pred[idx,0]), 'predicted responses:' , int(y_pred[idx,1]))
  print('---------')

print('\n\n\n')  
print('posts that is predicted to be ignored:')

for idx in [5201, 5738, 7608]:
  post = data_2_post.loc[idx][['title', 'score',	'comms_num']].to_list()
  print('title:', post[0],'score:', post[1],'responses:',post[2])
  print('\n')
  print('predicted score:', int(y_pred[idx,0]), 'predicted responses:' , int(y_pred[idx,1]))
  print('---------')

"""As we can see, the posts that are prediceted to be welcoming actually received lots of responses, and vice versa, though the magnitude of the prediction is exaggerate and inrealistic (because we are showing the extreme examples). And we can also observe that those hot posts are actually all about a specific and attracting topic, like complaining about advertisements or information about stock, etc.

The next model we tried is a multi layer neural net, which has three linear layers and uses prelu as the activation funcion, and batchnormalization to prevent overfit and backprop problems. To train the network, we used MSE (mean square error) loss and Adam optimizer with a initial learning rate of 0.01
"""

# neural net
# predict score/ number of response/ stock prize/ possibility of being removed

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset

class net(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(net, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.l1 = nn.Sequential(
            nn.BatchNorm1d(self.input_dim),
            torch.nn.Linear(input_dim, 128),
            nn.PReLU(),
            nn.BatchNorm1d(128),
         )
        self.l2 = nn.Sequential(
            torch.nn.Linear(128, 64),
            nn.PReLU(),
            nn.BatchNorm1d(64),
         )
        self.l3 = nn.Sequential(
            torch.nn.Linear(64, self.output_dim),
         )
        self.relu = nn.Sequential(
            nn.ReLU(inplace=True),
         )

    def forward(self, x):
        x = self.l1(x)
        x = self.l2(x)
        x = self.l3(x)
        # x = x.view(x.shape[0],-1)
        y_pred = self.relu(x)
        return y_pred


input_dim = X_train.shape[1]### fill in the input dim

# function to train a network
def train_model(model, optimizer, criterion, epochs=100, report_frequency = 1):
    
    model.to(device)
    model.train()

    loss_list = []
    for epoch in range(epochs):
      num = 0
      epoch_loss = None 
      for data, label in train_loader:
        data = data.to(device)
        label = label.to(device)
        pred = model(data)     
        loss = criterion(pred, label).sum()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        try:
          epoch_loss += loss.item()
        except:
          epoch_loss = loss.item()


        num += label.shape[0]

      loss_list.append(epoch_loss/num)
      if epoch%report_frequency ==report_frequency-1:
        print('epoch: ', epoch + 1, ' loss: ', epoch_loss/num)

    final_training_loss = loss_list[-1]
    return model, loss_list

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
criterion = torch.nn.MSELoss(reduce=False, size_average=False)

"""Build the dataloader and train the network for 100 epochs:"""

seed(1)

# train the model for 100 epochs
train_loader =  DataLoader(TensorDataset(torch.tensor(X_train).float(),torch.tensor(y_train).float()),shuffle = True,batch_size=512)
test_loader =  DataLoader(TensorDataset(torch.tensor(X_test).float(),torch.tensor(y_test).float()),shuffle = False,batch_size=128)

nn_model = net(input_dim, 2)
nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-2)

model, loss_list = train_model(nn_model, nn_optimizer, criterion, 100, 10)

"""The training loss curve:"""

epochs = torch.linspace(1,100,100)
plt.figure()
plt.plot(epochs, loss_list)
plt.xlabel('epochs')
plt.ylabel('training loss')
plt.title('training loss curve')
plt.show()

"""Test the model and check the correspondence between the prediction and the ground truth."""

# function to test a model

def test_model(model, criterion):
    model.eval()

    testing_loss = 0
    num = 0
    predcitons = []
    with torch.no_grad():
      for data, label in test_loader:
        data = data.to(device)
        label = label.to(device)
        pred = model(data)
        loss = criterion(pred, label).sum()
        testing_loss += loss.item()
        num += label.shape[0]
        predcitons.append(pred)
    testing_loss /= num
    return  testing_loss, torch.cat(predcitons)

testing_loss, predcitons = test_model(model, criterion)


corr_1 = np.corrcoef(predcitons[:,0],y_test[:,0])[0,1]
corr_2 = np.corrcoef(predcitons[:,1],y_test[:,1])[0,1]
print('Correlation coefficients of the predicted score a post will get and the ground truth:')
print(corr_1)
print('Correlation coefficients of the number of responses a post will get and the ground truth:')
print(corr_2)

"""As we can see, the model still learns little about the score of a post, but performs better in predicting the number of responses of a post (improved by about 0.06). We print out some examples."""

print('posts that is predicted to get a lot of responses:')

for idx in [7290, 2468, 5256]:
  print(idx)
  post = data_2_post.loc[idx][['title', 'score', 'comms_num']].to_list()
  print('title:', post[0],'score:', post[1],'responses:',post[2])
  print('\n')
  print('predicted score:', int(predcitons[idx,0]), 'predicted responses:' , int(predcitons[idx,1]))
  print('---------')

print('\n\n\n')  
print('posts that is predicted to be ignored:')

for idx in [  0, 5870, 5869]:
  post = data_2_post.loc[idx][['title', 'score', 'comms_num']].to_list()
  print('title:', post[0],'score:', post[1],'responses:',post[2])
  print('\n')
  print('predicted score:', int(predcitons[idx,0]), 'predicted responses:' , int(predcitons[idx,1]))
  print('---------')

"""As we can see, two models actually gets quite similar results, as the attracting topics about stocks and phelosophy gets high predictions, and adverts and long boring stories get poor predictions.

Now we look at the dataset1, the larger dataset which have about 1 million posts from about 470 thousand authors. For this dataset we analized the interrelationship among five of its featurs: number of post from an author, the average score, comments the author get per post, how many of his/her post were removed and the average award that the author get per post.

Fristly lets glance at the dataset, it can be seen that most authors actually only posted 1 post.
"""

data_1_author

"""We now want to study how many awards will someone get from a post given his history record of followers, score, etc."""

# predcite how many awards will an author receive

features = data_1_author[['num_post', 'mean_score', 'mean_comms','ratio_removed']]

label = data_1_author['mean_awards']

# the dataset is toooooo large so I only used part of it, otherwise the grid search will take literaly a year
X_train,X_test,y_train,y_test = train_test_split(features,label,test_size=0.1,train_size=0.1, random_state = 0)

print(X_train.shape)

"""I used SVR to perform the prediction, and used grid search to find the best set of hyperparameters. This is super time-costy as it took me hours to run."""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

model = GridSearchCV(SVR(), param_grid={"kernel": ("linear", 'rbf', 'sigmoid'), "C": np.logspace(-3, 3, 3), "gamma": np.logspace(-3, 3, 3)})
model.fit(X_train, y_train)

y_pre = model.predict(X_test)

corr = np.corrcoef(y_pre,y_test)[0,1]

print('Correlation coefficients of the prediction and the ground truth:')
print(corr)
print('The mean square error of the prediction')
print(np.sum(np.square(y_pre-y_test))/y_test.shape[0])

"""As we can see, the prediction is to some extent have the same trend with the ground truth, but the correlation coefficients suggests that the result is far from accurate. Here are some of the examples:"""

import random

i = 0
j = 0
while(i+j < 10 and i< 8):
  idx = random.randint(0,y_test.shape[0]-1)
  if y_test.to_numpy()[idx]> 0:
    print('ground truth:', y_test.to_numpy()[idx], 'prediction:', y_pre[idx])
    i += 1
  elif j<2:
    print('ground truth:', y_test.to_numpy()[idx], 'prediction:', y_pre[idx])
    j += 1

"""As we can see, the result is kind of inaccurate, so i looked back into the dataset:"""

import random

for i in range(10):
  idx = random.randint(0,y_test.shape[0]-1)
  print('ground truth:', y_test.to_numpy()[idx], 'prediction:', y_pre[idx])

"""It seems that there most authors acutually never got any award, so the data is super inbalance. In this case we manually balanced the data and run the model again."""

import random

i = 0
j = 0
idxlist = []

X_train,X_test,y_train,y_test = train_test_split(features,label,test_size=0.1,train_size=0.9, random_state = 0)

posidx = np.where(y_train.to_numpy()>0)[0]
num = posidx.shape[0]

print('number of positive examples:', num)

# so we select 4000 examples each
while(i < 4000 and i< 4000):
  idx = random.randint(0,y_train.shape[0]-1)
  if y_train.to_numpy()[idx]> 0 and i < 4000 and idx not in idxlist:
    idxlist.append(idx)
    i += 1
  if y_train.to_numpy()[idx]== 0 and j < 4000 and idx not in idxlist:
    idxlist.append(idx)
    j += 1

X_train = X_train.to_numpy()[idxlist]
y_train = y_train.to_numpy()[idxlist]

"""Train the model again:"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

model = GridSearchCV(SVR(), param_grid={"kernel": ("linear", 'rbf', 'sigmoid'), "C": np.logspace(-3, 3, 3), "gamma": np.logspace(-3, 3, 3)})
model.fit(X_train, y_train)

y_pre = model.predict(X_test)

corr = np.corrcoef(y_pre,y_test)[0,1]

print('Correlation coefficients of the prediction and the ground truth:')
print(corr)
print('The mean square error of the prediction')
print(np.sum(np.square(y_pre-y_test))/y_test.shape[0])

"""The result looks much better than it was before the balance.

And also, I applied linear regression (because its computational cheaper and we are running out of time lol. It tooks hours for us to run a neural net on such a big dataset each time) to predict whether a post will be removed given its score, number of comments, etc. Sigmoid is applied here to make the prediction a valid possibility.
"""

# logistic regression
# predict whether an author's post will be removed (1 for 100 precent, 0 otherwise)

from sklearn.linear_model import LinearRegression

lr=LinearRegression()
lr.fit(X_train,y_train)

y_pre=torch.sigmoid(torch.Tensor(lr.predict(X_test)))
y_pre = np.array(y_pre)

corr = np.corrcoef(y_pre,y_test)[0,1]
print('Correlation coefficients of the prediction and the ground truth:')
print(corr)
print('The mean square error of the prediction')
print(np.sum(np.square(y_pre-y_test))/y_test.shape[0])

"""As we can see, the linear regression model performs poorly even if it is using the same set of data, which suggests that the data has a strong non-linearity

### **Difficulties**

We also tried to extract the information from the titles of dataset 1 and do some exciting thing, such as find the relationship between the keyword frequency (which contains stock name) and the stock prize over time, however as the dataset is overwhelmingly large we find it too time-consuming and computational costy to do predictions with the word frequency of dataset 1. I acutually experienced stack overflow when I tried to apply adaboost on it to predict whether a post will be removed. 

The following codes are part of the exploration we tried, and we dicied to just leave it this way because of time. Given better computaiton resorce (like colab pro) and one more week, we might be able to do some super exciting thing with it.
"""

X = data_1_not_del['title']

vectorizer = TfidfVectorizer(max_df=0.1, min_df=0.0005, stop_words=all_stopwords)
features = vectorizer.fit_transform(X)
words = vectorizer.get_feature_names()

print(words)
print(len(words))

label = data_1_not_del[['removed']]

X_train,X_test,y_train,y_test = train_test_split(features,label,test_size=0.2,train_size=0.8, random_state = 0)

del features

# Adaboost
# predict whether a post will be removed (1 for removed, 0 otherwise)
# hyperparameter: max_depth, min_samples_split, min_samples_leaf, algorithm, n_estimators, learning_rate

bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3, min_samples_split=20, min_samples_leaf=5),
                         algorithm="SAMME",
                         n_estimators=200, learning_rate=0.8)
bdt.fit(X_train, y_train)

pred = bdt.predict(X_test)

corr = np.corrcoef(y_pre[:,0],y_test)[0,1]
print(corr)

print(torch.nn.MSELoss(y_pre, y_test))

"""##**Future Implementation**
We wanted to tried out rnn model, which was specilized in analyzing the texts and is believed to give more accurate predictions. However we found it really hard to implement, especially for starters like us. For future implementations, we think rnn would be the correct direction to work on, and it would also be interesting to focus on stock price verses post about stocks over time. 
"""